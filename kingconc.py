# -*- coding: utf-8 -*-
"""kingconc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LilN_l1nuujO3xSujKHeDQeIZ2esK5hW

# **CÓMO FUNCIONA**

Para crear un objeto solo tenemos que dárselo como argumento. Por ejemplo:

- Corpus(texto)
- Target(texto)/Reference(texto)
- Palabra(texto)

Luego sobre este objeto podremos realizar las distintas funciones asociadas a cada objeto.

Para que el texto que le pasemos al objeto sea más de uno, utilizamos la función crear_corpus() y le damos como argumento una lista de rutas de los archivos que queremos abrir.

Podemos representar algunas de las funciones que tienen asociadas sus frecuencias mediante wordcloud y exportar a .csv o excel aquellas que dan como resultado un dataframe.

# **PASO 0** PSEUDOCÓDIGO

0. **inicio**
1. Instalamos e importamos lo que necesitemos para la creación de las distintas funciones. Vamos a usar NLTK, spaCy, Stanford Core NLP y log de math para la log-likelihood.
2. Creamos una función para leer documentos, abrirlos
3. La función anterior la usamos en una nueva función que sirve para crear un corpus de más de un documento. Esta función recibe una serie de rutas y con ellas va abriendo los archivos en esa ruta, añadiéndolos a una variable vacía llamada corpus. Al final, nos devuelve esa variable con los documentos en formato string.
4. Creamos una clase llamada Corpus, cuyo atributo principal y obligatorio será el texto. Esto quiere decir que para poder crear un objeto Corpus, debemos pasarle sí o sí un texto.
  
  - A este objeto le asignamos unos atributos opcionales, que iremos rellenando con las distintas funciones.
  - Creamos una serie de funciones que podamos aplicar al texto del objeto Corpus. Una será de limpieza con expresiones regulares, otra de tokenización, otras dos de tokenizar pero usando los lemas en lugar de las palabras originales del texto, otra para calcular la frecuencia de los tokens, otras dos para eliminar las stopwords (una quitando las stopwords para el español del NLTK y otra quitando las stopwords que añada el usuario como argumento), y una última función extra de diversidad léxica.

5. Creamos dos subclases de Corpus, una para el corpus de referencia y otra para el target. Las dos heredan todas las funciones y atributos de su clase superior, Corpus, pero solo el Target tiene funciones y atributos extra, únicos para esta subclase.

  - Tenemos una función de keywords, una función de lista de palabras y una función de n-gramas

6. Creamos otra subclase, esta vez del objeto Target, que se llama Palabra. Hereda por tanto todo lo que tenía Target y todo lo que tenía Corpus. Para crear esta subclase hay que añadirle como argumentos un corpus target y el corpus de referencia.

  - Tenemos funciones para encontrar las concordancias de esa palabra en el target, otra en el de referencia, palabras con el mismo contexto, el análisis de las keywords (sus frecuencias y su keyness), las colocaciones en las que aparece inserta, los plot de dispersión en el target y el reference, encontrar los n-gramas de los que forma parte la palabra y los clústers.

7. Creamos una función que nos permita representar en una wordcloud las palabras y sus frecuencias.
8. **fin**

# **PASO 1** Preparación de las dependencias
"""

#Instalamos las librerías necesarias, en este caso nltk puede hacer la mayoría de cosas que queremos hacer
!pip install nltk
#para los lematizadores vamos a descargar dos posibilidades para ofrecer al usuario, Stanford Core NLP y Spacy
!pip install stanza
!pip install spacy
!python -m spacy download es_core_news_sm

#Importamos lo que necesitamos de nltk, y una librería que ya viene instalada en Colab, matplotlib, que sirve para gráficos
import nltk
from nltk import FreqDist
from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder
from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.text import Text
import matplotlib.pyplot as plt
import pandas as pd

#Descargamos de nltk lo que necesitamos para tokenizar y eliminar stopwords
nltk.download('stopwords')
nltk.download('punkt')

import stanza
stanza.download('es')
import spacy
import re
from math import log

from nltk.util import ngrams
from collections import Counter

"""# **PASO 2** Convertir varios documentos en corpus"""

#Vamos a crear unas funciones que nos permita trabajar luego con los textos

#Función para abrir/leer los documentos
def leer_doc(direccion):
  with open(direccion, 'r', encoding='utf-8') as f: #abrimos el documento en modo lectura (r) y en codificación utf-8 para que poder trabajar con él
    doc=f.read() #leemos el archivo
  return doc #queremos que esta función nos devuelva el archivo ya abierto, solo para lectura, no para modificarlo (modo escritura)

#Función para crear un corpus de más de un documento
def crear_corpus(num_docs, lista_paths):
  corpus=""
  for x in range(num_docs):
    for path in lista_paths:
      texto_corpus=leer_doc(path)
      corpus=f"{corpus}\n{texto_corpus}"
  return corpus

"""# **PASO 3** La clase "Corpus" y las funciones básicas asociadas al corpus

Aquí tenemos una clase, llamada Corpus, que tiene asociadas varias funciones. Esto significa que solo podrás usar estas funciones con un objeto que sea un Corpus. Para convertir un texto tuyo a Corpus tendrás que usarlo como una función:
>texto_convertido_a_corpus=**Corpus(**textomio**)**
"""

class Corpus:
  def __init__(self, text): #para crear un objeto Corpus, inicializarlo, recibe un argumento
    self.text=text #ese mismo argumento tiene un atributo de texto que es el mismo texto que recibe como argumento, es opcional, por eso el None
    #ese None significa que no tenemos obligatoriamente que darle un valor a este atributo para poder crear un Corpus()
    self.doc=None #este atributo va a ir variando con las diferentes funciones de abajo, pero principalmente va a recoger el texto tokenizado
    self.tokens=None #aquí vamos a guardar únicamente el texto tokenizado sin ninguna otra función aplicada
    self.tokenlema_spacy=None #aquí vamos a guardar los lemas por si acaso queremos acceder a ellos en algún momento sin cambios de expresiones regulares y tal
    self.tokenlema_stanford=None #lo mismo con los lemas de stanford core nlp
    self.frec=None #aquí guardamos el diccionario de frecuencias de los tokens en el corpus
    self.diversilex=None #aquí guardamos

  #-----FUNCIÓN PARA LIMPIAR CON EXPRESIONES REGULARES EL TEXTO-----

  #Función que utiliza expresiones regulares (regex) para quitar todo aquello que no sean números y palabras del texto
  def limpieza_regex(self):
    if isinstance(self.text, str): #si el texto es una string
        self.doc=re.sub(r'[^a-zA-Z0-9\sñáéíóúü]', '', self.text) #sustituimos por una cadena vacía todo aquello que no sea alfanumérico o un espacio en blanco
    elif isinstance(self.text, list): #si es una lista el texto
        texto=" ".join(self.text) #lo unimos para convertirlo en string
        self.doc=re.sub(r'[^a-zA-Z0-9\sñáéíóúü]', '', texto) #y hacemos la sustitución anterior
    else: #si no se cumple nada de lo anterior
        raise TypeError("El tipo de datos del texto no es válido.") #lanzamos un error de tipo de dato, tendría que haberse dado como string o lista
    return self #devolvemos el mismo tipo de objeto para poder seguir aplicando funciones

  #-----FUNCIONES PARA TOKENIZAR O LEMATIZAR EL TEXTO-----

  #Función de tokenización
  def tokenizar(self):
    self.tokens=nltk.word_tokenize(self.doc) #en el atributo tokens guardamos el texto tokenizado por el nltk
    self.doc=self.tokens #actualizamos el atributo doc con el texto tokenizado
    return self #nos devuelve el mismo objeto para que podamos seguir aplicando funciones

  #Función de lematización con spaCy
  def tokenlema_spacy(self):
    pln=spacy.load('es_core_news_sm') #cargamos el modelo para el español de spacy
    documento = pln(self.text) #convertimos el texto en un objeto de spacy para sacar su atributo lema
    self.tokenlema_spacy=[str(palabra.lemma_) for palabra in documento] #por cada palabra en la variable documento, nos quedamos con su lema y lo guardamos en el atributo lemas_spacy
    self.doc=self.tokenlema_spacy
    return self #nos devuelve el mismo objeto para poder seguir realizando funciones sobre él

  #Función de lematización con Stanford Core NLP
  def tokenlema_stanford(self):
    nlp = stanza.Pipeline(lang='es', processors='lemma') #cargamos el lematizador en español
    texto = nlp(self.text) #se lo pasamos al texto
    self.tokenlema_stanford = [word.lemma for sentence in texto.sentences for word in sentence.words] #guardamos en el atributo de lemas el lema de cada palabra
    self.doc=self.tokenlema_stanford #actualizamos el atributo doc con los tokens lematizados
    return self #nos devuelve el mismo objeto Corpus para poder seguir utilizando las funciones

  #-----FUNCIÓN DE CÁLCULO DE FRECUENCIA DE TOKENS-----

  #Función para la aplicación al doc tokenizado. Permite aplicar la función de frecuencia de tokens en un corpus
  def frecuencia_tokens(self):
    if self.doc is not None: #si en la variable documento hay algo
      self.frec=dict(FreqDist(self.doc))#asignamos al atributo frecuencia el resultado de aplicar la función frecuencia
      print(self.frec)
    else: #en caso contrario, pedimos al usuario que tokenize el texto antes de calcular la frecuencia de los tokens
      print("Error: no has tokenizado el texto")
    return self #nos devuelve el mismo objeto para poder seguir trabajando con él

  #-----FUNCIONES PARA ELIMINAR STOPWORDS----

  #Función que se aplica al doc tokenizado. Sirve para aplicar la función de quitar las stopwords en la lista de tokens con NLTK
  def autoeliminar_stopwords(self):
    if self.doc is not None: #si hay algo en la variable doc, o sea, si está el texto tokenizado
      sw=set(stopwords.words('spanish')) #nos quedamos con las stopwords del nltk en español únicas, que no estén repetidas
      stopwordsnltk=[palabra for palabra in self.doc if palabra.isalnum() and palabra not in sw]
      self.doc=stopwordsnltk #actualizamos la variable doc, pasándole la función swnltk (StopWords NLTK)
    else: #si no hay nada, advertimos al usuario de que aún no ha tokenizado el texto
      print("Error: no has tokenizado el texto")
    return self

  #Función que se aplica al doc tokenizado. Sirve para aplicar la función que nos permite quitar stopwords mediante un archivo propio
  def eliminar_stopwords(self, lista_stopwords=None):
    try: #probamos el siguiente código
      if self.doc is not None: #si hay algo en la variable doc, o sea, si el texto está tokenizado
        try: #prueba el siguiente código, para el que deben haber sido introducidas las stopword
          lista_stopwords1=lista_stopwords.split()
          sw=set(lista_stopwords1)
          stopwords=[palabra for palabra in self.doc if palabra.isalnum() and palabra not in sw]
          self.doc=stopwords #le pasamos la función para eliminar las stopword del grupo de tokens
          return self
        except TypeError: #si no funciona el código anterior, provocamos un error de tipo de dato
          print("Error: compureba que introdujiste las stopwords como una string.") #y avisamos al usuario del problema
      else:
        print("Error: no has tokenizado el texto") #si no hay nada en la variable doc, no se ha tokenizado el texto, imprimir aviso
    except: #si no funciona, imprimir el siguiente mensaje:
      print("Error: has olvidado incluir la lista de stopwords")

  #-----FUNCIONES EXTRA-----

  #Función que aplica para calcular la diversidad léxica del texto
  def diversidad_lexica(self):
    if self.doc is not None:
      vocabulario=len(set(self.doc))
      total=len(self.doc)
      self.diversilex=vocabulario/total
      return self
    else:
      print("Error: no has tokenizado el texto")

"""# **PASO 4** Las subclases del "Corpus": target y referencia

SUBCLASE DE CORPUS: REFERENCE
"""

class Reference(Corpus): #creamos una subclase de corpus, reference, para el corpus de referencia
  def __init__(self, text): #inicializamos la subclase
    super().__init__(text) #y hacemos que herede todos los atributos y funciones de la clase superior, Corpus

"""SUBCLASE DE CORPUS: TARGET"""

class Target(Corpus): #creamos una subclase de corpus
  def __init__(self, text): #para definir la clase necesitaremos pasarle un texto entre los paréntesis como argumento, tal que Target(texto)
    super().__init__(text) #le hacemos heredar los atributos y métodos de la clase superior, Corpus
    self.keywords_list=None #aquí vamos a guardar un diccionario de KEYWORD:KEYNESS, ordenado de mayor a menor, es opcional, por eso ponemos None
    #esto significa que no tiene que tener un valor obligatoriamente para que podamos crear un Target()
    self.keywords_results=None #aquí guardaremos un DataFrame con KEYWORD:FRECUENCIA TARGET:FRECUENCIA REFERENCIA:KEYNESS
    self.lista_palabras=None #aquí vamos a guardar un DataFrame de más a menos frecuente PALABRA:FRECUENCIA
    self.eneagramas=None #aquí guardaremos un DataFrame de N-GRAMA:FRECUENCIA

  #-----PRIMERO:FUNCIONES DE KEYWORDS-----

  #Esta función hace todo el análisis de las keywords: frecuencia en target, en reference, y la keyness, pero necesita un corpus de referencia
  def keywords_analisis(self, reference):
    try: #prueba este código, y si te da algún error, imprime el mensaje de except

      #---PRIMERO: CÁLCULO DE LA KEYNESS DE CADA TOKEN---
      #Vamos a extraer primero las keyword y su keyness, para ello necesitamos la frecuencia de cada token en el target corpus y reference corpus
      frecuencia_target=self.frec #Nos da un diccionario con palabra:frecuencia en el target corpus
      frecuencia_reference=reference.frec #sacamos las frecuencias de las palabras en el reference corpus

      #sumamos todas las frecuencias de cada corpus para obtener el total de palabras en cada uno de ellos
      totalfrec_target=sum(frecuencia_target.values())
      totalfrec_reference=sum(frecuencia_reference.values())
      #dejamos un diccionario vacío con el que vamos a crear luego un dict keyword:keyness
      keywords_list0={}

      #calculamos la log-likelihood
      for x in frecuencia_target: #por cada llave del diccionario de frecuencias, o sea, las palabras del corpus target
        if x in frecuencia_reference: #si esta palabra está también en el corpus reference, vemos la frecuencia de las distintas posibilidades:
          fpt=frecuencia_target[x] #de la palabra en el corpus target
          fpr=frecuencia_reference[x] #de la palabra en el corpus reference
          frt=totalfrec_target-fpt #del resto de palabras en el corpus target, sin la palabra que estamos mirando
          frr=totalfrec_reference-fpr #del resto en el corpus reference, sin la palabra que estamos mirando

          loglike_expected= 2*(fpt*log(fpt)+fpr*log(fpr)+frt*log(frt)+frr*log(frr)) #hacemos la log-likelihood de lo esperado, de la frecuencia de todo
          loglike_observed= 2 * ((fpt + fpr) * log(fpt + fpr) + (fpt + frt) * log(fpt + frt) + (fpr + frr) * log(fpr + frr) + (frt + frr) * log(frt + frr)) #calculamos la frecuencia de lo que hemos visto en cada corpus
          keyness=loglike_expected-loglike_observed #restamos a lo esperado, lo observado, obteniendo la keyness
          keywords_list0[x]=keyness #creamos una llave en el diccionario a la que asociamos su keyness

      klist=dict(sorted(keywords_list0.items(), key=lambda x:x[1], reverse=True)) #las ordenamos en reverse, esto es en valor descendente,
      #y en key indicamos que se fije en el segundo valor, la keyness para ordenarlas; y le asignamos este dict al atributo keywords_list
      self.keywords_list=pd.DataFrame(klist, columns=["Keyword", "Keyness"])

      #---SEGUNDO:AHORA QUE TENEMOS KEYWORDS ORDENADAS POR KEYNESS, LE DAMOS UN FORMATO DATAFRAME A TODOS LOS DATOS---
      #creamos una variable vacía a partir de la cual crearemos un DataFrame imprimible o exportable a excel
      datos=[]
      #lo llenamos con las keyword y sus datos asociados, como ya está en orden descendente el dict, nos aparecerán ordenadas por keyness y no frecuencia
      for x in klist.keys(): #por cada key, o sea keyword, en el diccionario keyword:keyness
        keyword=x #nos quedamos con la keyword
        frecT=frecuencia_target[x] #su frecuencia en el target
        frecR=frecuencia_reference[x] #su frecuencia en el reference
        knss=klist[x] #y su keyness
        datos.append([keyword, frecT, frecR, knss]) #lo añadimos a la lista de datos

      self.keywords_results=pd.DataFrame(datos, columns=["Keyword", "Frecuencia en target", "Frecuencia en referencia", "Keyness"]) #en el atributo
      #keywords_results guardamos una tabla de pandas con todos estos datos

      print(self.keywords_results) #imprimimos los resultados del análisis de las keyword

      return self #la función nos devuelve el mismo objeto para poder seguir operando con las distintas funciones sobre él
    except Exception as e: #imprime el siguiente aviso al usuario sobre porqué puede haberse dado el error
      print(f"Error: {e}")

  #-----SEGUNDO: FUNCIÓN DE WORDLIST-----

  #La primera función de esta sección de wordlist
  def wordlist(self):
    frecuencia_target=self.frec #calculamos la frecuencia de las palabras del target corpus (self)
    #Creamos un dataframe con las palabras y los valores de frecuencia para poderlo imprimir o exportar a excel:
    wlist=sorted(frecuencia_target.items(), key=lambda x:x[1], reverse=True) #organizamos por valores de frecuencia las palabras
    self.lista_palabras=pd.DataFrame(wlist, columns=["Palabra", "Frecuencia en target"]) #creamos un DataFrame para poder imprimir o exportar
    print(self.lista_palabras)
    #Esta función nos devuelve un DATAFRAME con las palabras en una columna y la frecuencia en el target
    return self

  #-----TERCERO: N-GRAMAS-----

  #función para sacar los enegramas ordenados por frecuencia de mayor a menor
  def enegramas(self, n): #a esta función tenemos que indicar el número n de los n-gramas
    #generamos los n-gramas con la función de ngramas de python
    ngramas=list(ngrams(self.doc, n))

    #calculamos la frecuencia de cada n-grama con la función counter()
    frecenegramas=Counter(ngramas)

    #transformamos el counter en diccionario para poder pasarlo a DataFrame e imprimir los resultados en tabla o exportarlos a excel
    para_tabla=dict(frecenegramas)

    #ordenamos los n-gramas por frecuencia de mayor a menor
    resultados=sorted(para_tabla.items(), key=lambda x:x[1], reverse=True)

    #lo convertimos a DataFrame para poder imprimirlo o exportarlo a excel
    self.eneagramas=pd.DataFrame(resultados, columns=["N-gramas", "Frecuencia en target"])
    print(self.eneagramas)
    #esta función nos devuelve un DataFrame con los n-gramas ordenados por frecuencia
    return self

"""# **PASO 5** Subclase de "Target", la "Palabra"

SUBCLASE DEL TARGET: LA PALABRA
"""

class Palabra(Target): #creamos una subclase de Target, llamada Palabra, que tendrá todos los métodos tanto de Corpus como de Target
  def __init__(self, text, target, reference):
    super().__init__(text)
    self.target=target
    self.reference=reference
    self.keyness_value=None
    self.onekeyword_results=None
    self.colocaciones=None
    self.eneagramas_palabra=None

  #-----PRIMERO: FUNCIONES DE KEYWORDS-----

  #Esta función nos permite calcular la keyness de una palabra concreta dado el target y el reference corpus
  def calcular_keyness(self):
    try:
      palabra=self.text.lower() #convertimos la palabra, por si acaso, en minúscula
      keywords=self.target.keywords_list #guardamos el dict KEYWORD:KEYNESS
      keywords2={key.lower():value for key, value in keywords.items()} #pasamos a minúscula las keys de los pares llave:valor con un dict de comprensión
      self.keyness_value=keywords2[palabra] #sacamos la keyness de la palabra concreta que queremos en minúscula
      print(f"La keyness de {self.text} es {self.keyness_value}") #imprimimos la keyness de la palabra
      return self #devuelve el mismo objeto
    except Exception as e:
      print(f"Error: {e}")

  #Esta función nos permite sacar las frecuencias en los corpus y la keyness de una keyword
  def onekeyword_analysis(self):
    try: #prueba este código, si no funciona, imprime el aviso al usuario que hay en except
      palabra=self.text
      frecuencia_target=self.target.frec #calculamos la frecuencia de cada una de las palabras del target
      frecuencia_reference=self.reference.frec #calculamos la frecuencia de cada una de las palabras del reference
      kns=self.calcular_keyness() #calculamos la keyness de la palabra
      resultados={'Keyword':palabra, 'Frecuencia en target': frecuencia_target.get(palabra, None), 'Frecuencia en reference': frecuencia_reference.get(palabra, None), 'Keyness': kns} #guardamos el resultado en un diccionario
      self.onekeyword_results=resultados #le asignamos lo anterior al atributo self.onekeyword_results
      print(self.onekeyword_results)
      return self #esta función nos devuelve un DICCIONARIO con la keyword, frecuencias y keyness de una palabra que escojamos
    except Exception as e: #imprimimos un aviso para el usuario sobre lo que puede haber ido mal
      print(f"Error: {e}")

  #-----SEGUNDO: CONCORDANCIAS-----

  #Esta función se aplica a la palabra que queremos y recibe como argumento el target, y nos da las concordancias de la palabra
  def conc_target(self):
    try: #prueba este código y en caso de no funcionar, imprime el mensaje en except
      if self.target.doc is not None: #si el atributo doc del target no está vacío, haz lo explicado en el siguiente código:
        tokens=self.target.doc #tokenizamos el corpus target
        tx=nltk.Text(tokens) #creamos un objeto de Texto de nltk que tiene entre sus funciones la de concordancia
        contextos=tx.concordance(self.text) #guardamos en una variable el resultado de la función de concordancia, a la que le pasamos el self, la palabra que queremos mirar
        return self #la función nos devuelve una string con los contextos de la palabra
      else:
        print("Error: no has tokenizado el target corpus.") #imprime un aviso si el doc está vacío
    except Exception as e:
      print(f"Error: {e}")

  #Esta función saca las concordancias de la palabra en el corpus de referencia
  def conc_reference(self):
    try:
      if self.reference.doc is not None:
        tokens=self.reference.doc
        tx=nltk.Text(tokens) #creamos un objeto de texto de nltk para poder sacar las concordancias
        contextos=tx.concordance(self.text) #sacamos los contextos de la palabra
        return self #la función nos devuelve una string con los contextos de la palabra en el corpus de referencia
      else:
        print("Error: falta tokenizar el corpus de referencia.")
    except Exception as e:
      print(f"Error: {e}")

  #-----TERCERA: MISMO CONTEXTO-----

  #Esta función que necesita el target y el reference y nos va a decir las palabras que, en cada uno, aparecen en los mismos contextos
  def mismo_cx(self):
    try:
      if self.target.doc is not None and self.reference.doc is not None:
        tokens1=self.target.doc #tokenizamos el corpus target
        tokens2=self.reference.doc #tokenizamos el corpus de ref
        tx1=nltk.Text(tokens1) #convertimos en objeto de texto de nltk el corpus target
        tx2=nltk.Text(tokens2) #convertimos en objeto de texto de nltk el corpus de ref
        samesies1=tx1.similar(self.text) #sacamos las palabras que aparecen en contextos similares en el target corpus
        samesies2=tx2.similar(self.text) #sacamos las palabras que aparecen en contextos similares en el corpus de referencia
        resultado=f"Las palabras con el mismo contexto que {self.text} en el corpus target son:\n{samesies1}\nLas palabras con el mismo contexto en el corpus de referencia son:\n{samesies2}\n"
        print(resultado)
      else:
        print("Error: no has tokenizado el target corpus y/o el reference corpus.")
      return self
    except Exception as e:
      print(f"Error: {e}")

  #-----CUARTA: COLOCACIONES-----

  #Esta función saca las colocaciones con la palabra
  def encontrar_colocaciones(self):
    try:
      tokens=self.target.text
      tx=Text(tokens) #creamos un objeto de texto con el texto del corpus target
      colocs=tx.collocation_list() #sacamos la lista de colocaciones
      word=self.text #guardamos la string de la palabra
      colocs_con_word = [coloc for coloc in colocs if word in coloc] #nos quedamos con las colocaciones que contienen nuestra palabra
      colocs_counts = Counter(colocs_con_word)  # Count frequency of collocations
      self.colocaciones = pd.DataFrame({"Colocaciones": list(colocs_counts.keys()), "Frecuencia en target": list(colocs_counts.values())})
      print(self.colocaciones) #imprimimos el dataframe anterior como resultado
      return self #la función nos devuelve el mismo objeto para poder seguir usando las funciones
    except Exception as e: #si tenemos un error, imprime el error tal cual en lugar de dar la petada
      print(f"Error: {e}")

  #-----QUINTA: PLOT DE DISPERSIÓN-----

  #Función de representación de la dispersión de las palabras en el target y el corpus de referencia
  def plot(self):
    ta=self.target.text #sacamos el atributo de texto de los dos corpus que hemos recibido como argumento, el target y el reference
    re=self.reference.text
    t=Text(ta) #creamos dos objetos de texto del NLTK que nos permitirán aplicar ciertas funciones, como la de plot de dispersión
    r=Text(re)
    print("Estos son el plot de dispersión en el corpus target y en el corpus de referencia:\n") #imprimimos un mensaje de aviso para que sepan que 1º
    #va la dispersión del corpus target, y el 2º el corpus de referencia
    t.dispersion_plot([self.text]) #sacamos el plot de dispersión de la palabra en los objetos de texto que hemos creado antes
    r.dispersion_plot([self.text])
    return self #nos devuelve el mismo objeto para que podamos seguir usando funciones con él

  #-----SEXTA: N-GRAMAS QUE INCLUYEN UNA PALABRA CONCRETA-----

  #Esta función encuentra los n-gramas que incluyen la palabra
  def encontrar_ngramas(self, num):
    if self.target.doc is not None:
      tokens=self.target.doc #guardamos el corpus target tokenizado
      todos_ngramas=list(ngrams(tokens, num)) #sacamos los n-gramas con el número recibido como argumento y los tokens del corpus target
      n_gramas_palabra=[x for x in todos_ngramas if self.text in x] #en una lista de comprensión sacamos todos los n-gramas que contengan la palabra (self.text)
      frecuencia=Counter(n_gramas_palabra) #creamos un diccionario con los n-gramas y su frecuencia
      ngramas_ordenados = sorted(frecuencia.items(), key=lambda x: x[1], reverse=True) #lo organizamos de mayor a menor por la frecuencia
      self.eneagramas_palabra=pd.DataFrame(ngramas_ordenados) #creamos un dataframe imprimible o exportable
      print(self.eneagramas_palabra)
      return self
    else:
      print("Error: no has tokenizado el corpus target.")

  #-----SÉPTIMA: CLÚSTERS-----
  def encontrar_clusters(self, ventana):
    #tokenizamos el texto
    tokens = nltk.word_tokenize(self.target.text.lower()) #tokenizamos el texto del corpus target en minúscula todo

    #por cada índice y token en los tokens enumerados, si el token es igual a la palabra en minúscula, nos quedamos su índice, o sea, posición
    indices = [i for i, token in enumerate(tokens) if token == self.text.lower()]
    #creamos una lista vacía para almacenar las colocaciones (palabras que aparecen junto al patrón) y sus frecuencias
    word_contexts = []
    #por cada índice y palabra en la enumeración de los tokens:
    for i, word in enumerate(tokens):
        if word == self.text.lower(): #si la palabra es igual al atributo texto en minúscula
            start_index=max(0, i - ventana) #le restamos al índice la ventana, para obtener la posición inicial donde debería empezar el cx
            end_index=min(len(tokens), i + ventana + 1) #luego le añadimos la ventana y 1 (la palabra), al índice de la palabra, para saber el último índice
            context=' '.join(tokens[start_index:i+1])  #juntamos los tokens para formar el contexto de la izquierda
            word_contexts.append(context) #lo añadimos a la lista vacía
            context2=' '.join(tokens[i:end_index]) #cogemos la palabra y el contexto de detrás
            word_contexts.append(context2) #la añadimos a la lista vacía

    #ahora contamos la frecuencia de cada contexto/clúster
    cluster_counts = {x:word_contexts.count(x) for x in word_contexts} #creamos un diccionario con cada PALABRA:FRECUENCIA

    # Sort by frequency
    sorted_cluster_counts = sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True) #ORGANIZAMOS DE MAYOR A MENOR POR FRECUENCIAS

    # Return a list of tuples containing the pattern, collocated word, and frequency
    clusters_final=pd.DataFrame(sorted_cluster_counts, columns=["Clústers", "Frecuencia en target"])
    print(clusters_final)
    return self

"""# **PASO 6** Función de representación en wordcloud y exportar DataFrames a excel/csv"""

#Función de representación en wordcloud

#importamos la clase wordcloud de la librería wordcloud, lo que nos va a permitir crear wordclouds
from wordcloud import WordCloud
#creamos una función de generación de wordclouds que toma como argumento un diccionario de frecuencias
def generar_wordcloud(dicc_frecuencias):
  if isinstance(dicc_frecuencias, dict):
    wc=WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(dicc_frecuencias) #creamos un objeto wordcloud y hacemos que se genere a partir del dicc de frecuencias

    plt.figure(figsize=(10,5)) #especificamos el tamaño del cuadro de la wordcloud
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off") #no necesitamos eje de coordenadas
    plt.show()

  elif isinstance(dicc_frecuencias, pd.DataFrame):
    d=dicc_frecuencias.to_dict(orient='dict') #lo convertimos en diccionario si es un dataframe
    wc=WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(d) #creamos un objeto wordcloud y hacemos que se genere a partir del dicc de frecuencias

    plt.figure(figsize=(10,5)) #especificamos el tamaño del cuadro de la wordcloud
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off") #no necesitamos eje de coordenadas
    plt.show()

#Función para exportar a csv o excel

def exportar_dataframe(dataframe, filename, extension):
  nombre=filename
  if 'excel' or 'xlsx' in extension.lower():
    dataframe.to_excel(nombre, index=False)
    print(f"{nombre}.xlsx ha sido exportado correctamente.")
  elif 'csv' in extension.lower():
    dataframe.to_csv(nombre, index=False)
    print(f"{nombre}.csv ha sido exportado correctamente")
  else:
    print("El formato introducido no es válido. Prueba con xlsx o csv.")

"""# **PRUEBAS DE FUNCIONAMIENTO**

## PRUEBAS DE OBJETO CORPUS
"""

corp=Corpus("El niño de trece años salta a la comba con su hermana en el patio del colegio. La niña de catorce años corre por el campo de su abuela. Ellos corren, saltan, juegan y piden pizza.")

corp2=corp.limpieza_regex()
print(corp2.doc)

corp1=corp2.tokenizar()

print(corp1.doc)

corp3=corp1.frecuencia_tokens()

"""## PRUEBAS SUBCLASES TARGET Y REFERENCE"""

ref=Reference("Cuántos niños hay en el patio, nadie lo sabe. Todo el mundo corre, corren mucho. Todos comen muchas patatas. Ellas ríen a menudo. Ellos comen pizza.")

a=Target("El niño de trece años salta a la comba con su hermana en el patio del colegio. La niña de catorce años corre por el campo de su abuela. Ellos corren, saltan, juegan y piden pizza.")

b=a.limpieza_regex()

c=b.tokenizar()

d=c.frecuencia_tokens()

e=d.wordlist()
print(e)

g=ref.limpieza_regex()

h=g.tokenizar()

j=h.frecuencia_tokens()

n_gramos=d.enegramas(2)
print(n_gramos)

kiwis=d.keywords_analisis(j)

"""## PRUEBA SUBCLASE PALABRA"""

word=Palabra("el", d, j)

word_contexts=word.conc_target()

word_contexts_ref=word.conc_reference()

word_analysis=word.onekeyword_analysis()

word_plot=word.plot()

word_eneagrams=word.encontrar_ngramas(2)

word_col=word.encontrar_colocaciones()

word_samecx=word.mismo_cx()

word_clust=word.encontrar_clusters(3)